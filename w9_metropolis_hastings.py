# +
# Copyright (c) 2020 Urbain Vaes. All rights reserved.
#
# This work is licensed under the terms of the MIT license.
# For a copy, see <https://opensource.org/licenses/MIT>.
# import time
import time
import numpy as np
import scipy.stats
import networkx as nx
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.animation as animation
# -

# +
mpl.rc('font', size=20)
mpl.rc('font', family='serif')
mpl.rc('figure', figsize=(16, 11))
mpl.rc('lines', linewidth=2)
mpl.rc('lines', markersize=12)
mpl.rc('figure.subplot', hspace=.3)
mpl.rc('figure.subplot', wspace=.1)
mpl.rc('animation', html='html5')
np.random.seed(0)

def Metropolis_Hastings(n, J, π, x0, q, q_sampler):
    # x0: Initial condition
    # n: Number of iterations
    # J: Number of particles

    # (Notice that, in fact, taking the minimum is not necessary)
    α = lambda x, y: np.minimum(1, π(y)*q(y, x) / (π(x)*q(x, y)))

    # Vector with trajectories
    x = np.zeros((n + 1, J), dtype=type(x0))

    # Initial condition
    x[0, :] = x[0, :] + x0

    for i in range(n):
        y = q_sampler(x[i])
        u = np.random.rand(J)

        # https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html
        x[i + 1] = np.where(u < α(x[i], y), y, x[i])

    return x
# -

# # Discrete state space
# We begin by illustrating the Metropolis-Hastings algorithm in the finite state space
# $\{0, \dotsc, N\}$, for some natural number $N$.

# +
# Number of particles
J = 10000

# Number of steps
n = 100

# Size of discrete state space
N = 24

# Discrete state space
S = np.arange(N, dtype=int)

# Desired stationary distribution
π = lambda x: 1/N * (1 + .9 * np.sin(2*np.pi*(x/N)))

# Independence sampler with uniform proposal
q_indep = lambda x, y: 1/N
q_indep_sampler = lambda x: np.random.randint(0, N, size=len(x))

# Random walk Metropolis-Hastings
q_rwmh = lambda x, y: (1/2) * (y == (x + 1) % N) + (1/2) * (y == (x - 1) % N)
q_rwmh_sampler = lambda x: (x + (1 - 2*np.random.randint(0, 2, size=J))) % N

# Initial condition
x0 = 0

# Metropolis-Hastings
x_rwmh = Metropolis_Hastings(n, J, π, x0, q_rwmh, q_rwmh_sampler)
x_indep = Metropolis_Hastings(n, J, π, x0, q_indep, q_indep_sampler)

def anim_pmf(x, n_steps=n):
    def plot_pmf(i, x, π):
        ax.clear()
        ax.set_title("Probability mass function at iteration ${}$".format(i))
        ax.set_xlabel("Node index")
        x_plot, y_plot = np.unique(x[i], return_counts=True)
        ax.stem(x_plot - .05, y_plot/len(x[0]), use_line_collection=True,
                label="Metropolis-Hastings runs", linefmt='C0-', markerfmt='C0o')
        x_plot = np.arange(N)
        ax.stem(x_plot + .05, π(x_plot), use_line_collection=True,
                label="Target distribution", linefmt='C1-', markerfmt='C1o')
        ax.set_ylim(0, 0.2)
        ax.legend(loc="upper right")
        # time.sleep(1)

    mpl.rc('figure', figsize=(12, 8))
    fig, ax = plt.subplots()
    fig.subplots_adjust(left=.1, bottom=.1, right=.98, top=.95)
    iterate = lambda i: plot_pmf(i, x, π)
    anim = animation.FuncAnimation(fig, iterate, np.arange(n_steps), interval=600,
                                   init_func=lambda: None, repeat=True)

    # For Python
    # plt.show()

    # For notebook
    plt.close(fig)
    return anim
# -
# # Independence sampler
# In the animation, we plot the evolution of the pmf of the Markov chain.
# We could have calculated the exact probability mass function (pmf) at each
# iteration exactly by constructing the transition matrix and left-multiplying
# by its transpose at each iteration, but here we opted for an estimation by
# a usual Monte Carlo, based on the observation that the pmf at a point $x$ and
# a time $n$ is simply $\mathbb E [\delta_{x}(X_n)]$, where $\{X_n\}$ is the
# Markov chain generated by the Metropolis-Hastings algorithm.

# +
anim_pmf(x_indep, n_steps=20)
# -
# # Random Walk Metropolis-Hastings
# Here we use a proposal corresponding to a random walk on the finite domain,
# with periodic boundary condition. More precisely, we use the proposal density (pmf)
# $$
# q(y \, | \, x) =
# \begin{cases}
#    1/2 \quad & \text{if $y = (x + 1) \% N$} \\
#    1/2 \quad & \text{if $y = (x - 1) \% N$}
# \end{cases}
# $$

# +
anim_pmf(x_rwmh)
# -

# Size of discrete state space
# N = 24
#
# # Desired stationary distribution
# S = np.arange(N, dtype=int)
# π = 1/N * (1 + .5 * np.sin(2*np.pi*(S/N)))
#
# # Verify that π is a pmf:
# assert abs(np.sum(π) - 1) < 1e-15
#
# # T is the transition matrix
# T = np.zeros((N, N))
# for i in range(N):
#     for j in range(N):
#         T[i, j] = p_indep(S[i], S[j])
#
# G = nx.DiGraph()
# for i, v in enumerate(T):
#     for j, n in enumerate(v):
#         if n != 0:
#             G.add_edges_from([(i, j)], weight=n)
#
# Δθ = 2*np.pi/N
# pos = {i: (np.cos(i*Δθ), np.sin(i*Δθ)) for i in range(N)}






# # Transition probability
# def transition(q, α):
#     def p(x, y):
#         result = q(x, y) * α(x, y)
#         if x == y:
#             for z in S:
#                 result += q(x, z) * (1 - α(x, z))
#         return result
#     return p

# # Independence sampler with uniform proposal
# g = (1/N) * np.ones(N)
# q_indep = lambda x, y: g[y]
# α_indep = lambda x, y: min(1, π[y]*g[x] / (π[x]*g[y]))
# p_indep = transition(q_indep, α_indep)


# values = np.zeros((N,))
# values[0] = 1
# labels = {j: v for j, v in enumerate(values)}

# fig, ax = plt.subplots()
# ax.set_xlim(-1.2, 1.2)
# ax.set_xlim(-1.2, 1.2)
# ax.set_aspect('equal')
# cmap = matplotlib.cm.get_cmap('viridis')
# nx.draw_networkx_labels(G, pos, labels=labels, font_size=16, ax=ax)
# nx.draw(G, pos, node_color=values, alpha=.5, node_size=3000,
#         # connectionstyle='arc3, rad=0.1',
#         ax=ax, cmap=cmap)
# plt.show()

# # Number of iterations
# n = 100

# # Number of particles
# J = 1000



# # values[i] contains the number of particles at the nodes at iteration i
# values = np.zeros((n + 1, N))
# values[0, 0] = 1
# exact = np.zeros((n + 1, N))
# values[0] = [N, 0, 0, 0, 0]
# exact[0] = [1, 0, 0, 0, 0]
# tr = np.array(T)

# # Simulation of the Markov chain
# for i in range(n):
#     for j, v in enumerate(T):
#         proposal =
#         for k in next_step:
#             values[i+1][k] += 1
#     exact[i+1] = tr.T.dot(exact[i])

# def plot_evolution(i):
#     ax.clear()
#     add_edges_labels(ax)
#     labels = {j: v for j, v in enumerate(values[i])}
#     nx.draw_networkx_labels(G, pos, labels=labels, font_size=16, ax=ax)
#     cmap = matplotlib.cm.get_cmap('viridis')
#     nx.draw(G, pos, node_color=values[i], alpha=.5, node_size=3000,
#             connectionstyle='arc3, rad=0.1', ax=ax, cmap=cmap)
#     ax.set_title("Discrete time: ${}$".format(i))

# def plot_pmf(i):
#     ax.clear()
#     ax.set_title("Probability mass function at iteration ${}$".format(i))
#     ax.set_xlabel("Node index")
#     ax.stem(np.arange(K) - .05, values[i]/N, use_line_collection=True,
#             label="MC approximation", linefmt='C0-', markerfmt='C0o')
#     ax.stem(np.arange(K) + .05, exact[i], use_line_collection=True,
#             label="Exact", linefmt='C1-', markerfmt='C1o')
#     ax.set_ylim(0, 1.1)
#     ax.legend()

# # For notebook
# # plt.close(fig)
# # return anim

# # -
